<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Weitai Kang</title>
  
  <meta name="author" content="Weitai Kang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Weitai Kang</name>
              </p>
              <p>
                I am a 4th-year Ph.D. student working with Prof. <a href='https://scholar.google.com/citations?user=zhi-j1wAAAAJ&hl=en'>Yan Yan</a> in Computer Science at the University of Illinois Chicago.
                I will graduate in May 2027.
                <br><br>
                I work on <b>Large Multimodal Models</b> across text, image, GUI, 3D, and video. 
                I specialize in Visual Grounding, Generalist Models, AI Agents, Reinforcement Learning, and Image Editing.
                I have published 14 papers, with <b>7 first-author papers at top-tier venues</b> (CVPR, NeurIPS, ICCV, ICLR, ECCV, and ACM MM).
                
		<br><br>
    I have interned at Netflix, Adobe, SonyAI, Tencent and SenseTime. I have been a Visiting Scholar at the University of Central Florida, working with Prof. <a href='https://scholar.google.com/citations?user=p8gsO3gAAAAJ'>Mubarak Shah</a>. Before starting my PhD, I received my bachelor's degree in Mathematics from Sun Yat-sen University in 2022, where I was awarded the Outstanding Student Scholarship each year.

		      
		<br><br>
              </p>
              <p style="text-align:center">
                <a href="mailto:k13711752197@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=hDl0MkwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/weitaikang/">Linkedin</a> &nbsp/&nbsp
                <a href="https://github.com/WeitaiKang">Github</a> &nbsp/&nbsp
                <a href="https://x.com/WeitaiKang">Twitter</a> &nbsp/&nbsp
		            <a href="experience.html">Hi~</a>
              </p>


        <br>
        <p style="color:red;">
          I am actively seeking full-time Research Scientist positions starting in 2027.
  Feel free to reach out if there is a good fit.
        </p>
        

            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/weitai.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/weitai.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>


        </tbody></table>


        
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Work Experiences</heading>
          </td>
        </tr>
        </tbody></table>
        
        <!-- <div style="width: auto; height: 450px; overflow: auto; border: 1px solid #ddd;"> -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/netflix.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Netflix Â· Research Internship</papertitle>
              <br><br>
              Research on Video Understanding.
              <br><br>
              May 2026 - Aug. 2026, Los Gatos, California, United States Â· On-site
              <br>
              <p></p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Adobe.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Adobe Â· Research Internship</papertitle>
              <br><br>
              Research on Visual Grounding and Image Editing.
              <br><br>
        May 2025 - Aug. 2025, San Jose, California, United States Â· On-site
              <br>
        Aug 2025 - May 2026, Chicago, Illinois, United States Â· Remote
              <br>
              <p></p>
            </td>
          </tr>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SonyAI.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>SonyAI Â· Research Internship</papertitle>
              <br><br>
              Research on 2D Large Multimodal Model.
              <br><br>
        Oct. 2024 - Dec. 2024, Chicago, Illinois, United States Â· Remote
              <br>
              <p></p>
            </td>
          </tr>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tencent.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Tencent Â· Machine Learning Engineer Internship</papertitle>
              <br><br>
              Work on Human Pose Detection.
              <br><br>
        Oct. 2021 - Jul. 2022, Shenzhen, Guangdong, China Â· On-site
              <br>
              <p></p>
            </td>
          </tr>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sensetime.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>SenseTime Â· Research Internship</papertitle>
              <br><br>
              Research on Video Super-Resolution.
              <br><br>
        Jul. 2021 - Sep. 2021, Shenzhen, Guangdong, China Â· On-site
              <br>
              <p></p>
            </td>
          </tr>
        
        </tbody></table>

        <br>
<!-- </div> -->
 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
            <!-- <p>
            </p> -->
          </td>
        </tr>
      </tbody></table>

      <div style="width: auto; height: 700px; overflow: auto; border: 1px solid #ddd;">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<!-- <tr bgcolor="#ffffd0"> -->

    <!-- <tr bgcolor="#ffffd0"> -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/vgent.png" alt="b3do" width="185" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        VGent: <b>Visual Grounding</b> via Modular Design for Disentangling Reasoning and Prediction
        <br>
        <i><u>Weitai Kang</u>, Jason Kuen, Mengwei Ren, Zijun Wei, Yan Yan, Kangning Liu</i>
        <br>
        <em style="color: red;">CVPR 2026</em>.
        <a href="https://www.arxiv.org/abs/2512.11099">PDF</a>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/GuirlVG.png" alt="b3do" width="185" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        GuirlVG: Incentivize <b>GUI Visual Grounding</b> via Empirical Exploration on <b>Reinforcement Learning</b>
        <br>
        <i><u>Weitai Kang</u>, Bin Lei, Gaowen Liu, Caiwen Ding, Yan Yan</i>
        <br>
        <em style="color: red;">ICLR 2026</em>.
        <a href="https://arxiv.org/abs/2508.04389">PDF</a>
      </td>
    </tr>


    <!-- <tr bgcolor="#ffffd0"> -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/Robin3D.png" alt="b3do" width="185" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          Robin3D: Improving <b>3D Large Language Model</b> via Robust Instruction Tuning
          <br>
          <i><u>Weitai Kang</u>, Haifeng Huang, Yuzhang Shang, Mubarak Shah, Yan Yan</i>
          <br>
          <em style="color: red;">ICCV 2025</em>.
          <!-- <br> -->
          <a href="https://arxiv.org/pdf/2410.00255">PDF</a> /
          <a href="https://github.com/WeitaiKang/Robin3D">Code</a>
          </td>
        </tr>

    

          
    <!-- <tr bgcolor="#ffffd0"> -->
      <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/Infant-next.png" alt="b3do" width="185" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                InfantAgent-Next: A Multimodal Generalist <b>Agent</b> for Automated Computer Interaction
                <br>
                <i>Bin Lei*, <u>Weitai Kang*</u>, Zijian Zhang, Winson Chen, Xi Xie, Shan Zuo, Mimi Xie, Ali Payani, Mingyi Hong, Yan Yan, Caiwen Ding</i>
                <!-- <br> -->
                <br>
                <em style="color: red;">NeurIPS 2025</em>.
                * Equal contribution.
                <!-- <br> -->
                <a href="https://arxiv.org/abs/2505.10887">PDF</a> /
                <a href="https://github.com/bin123apple/InfantAgent">Code</a>
              </td>
            </tr>
    

		
	 <!-- <tr bgcolor="#ffffd0"> -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/intent3d.png" alt="b3do" width="185" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          Intent3D: <b>3D Object Detection</b> in RGB-D Scans Based on <b>Human Intention</b>
          <br>
          <i><u>Weitai Kang</u>, Mengxue Qu, Jyoti Kini, Yunchao Wei, Mubarak Shah, Yan Yan</i>
          <br>
          <em style="color: red;">ICLR 2025</em>.
          <!-- <br> -->
          <a href="https://weitaikang.github.io/Intent3D-webpage/">Project Page</a> / 
              <a href="https://arxiv.org/abs/2405.18295">PDF</a> / 
              <a href="https://github.com/WeitaiKang/Intent3D">Code</a>
                  </td>
                </tr>
            

          <!-- <tr bgcolor="#ffffd0"> -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/Attbalance.png" alt="b3do" width="185" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                AttBalance: <b>Visual Grounding</b> with Attention-Driven Constraint Balancing
                <br>
                <i><u>Weitai Kang</u>, Luowei Zhou, Junyi Wu, Changchang Sun, Yan Yan</i>
                <br>
                <em style="color: red;">ACM MM 2025</em>.
                <!-- <br> -->
                <a href="https://arxiv.org/abs/2407.03243">PDF</a>
              </td>
            </tr>

            
          <!-- <tr bgcolor="#ffffd0"> -->
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SegVG.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              SegVG: Transferring Object Bounding Box to Segmentation for <b>Visual Grounding</b>
              <br>
              <i><u>Weitai Kang</u>, Gaowen Liu, Mubarak Shah, Yan Yan</i>
              <br>
              <em style="color: red;">ECCV 2024</em>.
              <!-- <br> -->
              <a href="https://arxiv.org/abs/2407.03200">PDF</a> /
              <a href="https://github.com/WeitaiKang/SegVG">Code</a>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MLLMVG.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              ExpVG: Investigating the Design Space of <b>Visual Grounding</b> in Multimodal Large Language Model
              <br>
              <i><u>Weitai Kang</u>, Weiming Zhuang, Zhizhong Li, Yan Yan, Lingjuan Lyu</i>
              <br>
              <a href="https://arxiv.org/abs/2508.08066">PDF</a>
            </td>
          </tr>
      
          
	        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/INTP.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              Interpolating <b>Video-LLMs</b>: Toward Longer-sequence LMMs in a Training-free Manner
              <br>
              <i>Yuzhang Shang, Bingxin Xu, <u>Weitai Kang</u>, Mu Cai, Yuheng Li, Zehao Wen, Zhen Dong, Kurt Keutzer, Yong Jae Lee, Yan Yan</i>
              <br>
              <a href="https://arxiv.org/abs/2409.12963">PDF</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ACTRESS.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              ACTRESS: Active Retraining for <b>Semi-supervised Visual Grounding</b>
              <br>
              <i><u>Weitai Kang</u>, Mengxue Qu, Yunchao Wei, Yan Yan</i>
              <br>
              <a href="https://arxiv.org/abs/2407.03251">PDF</a>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/3DResT.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              3DResT: A Strong Baseline for <b>Semi-Supervised 3D Referring Expression Segmentation</b>
              <br>
              <i>Wenxin Chen, Mengxue Qu, <u>Weitai Kang</u>, Yan Yan, Yao Zhao, Yunchao Wei</i>
              <br>
              <em style="color: red;">TMM 2025</em>.
              <a href="https://arxiv.org/abs/2504.12599">PDF</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Infant.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              Infant Agent: A Tool-Integrated, Logic-Driven <b>Agent</b> with Cost-Effective API Usage
              <br>
              <i>Bin Lei, Yuchen Li, Yiming Zeng, Tao Ren, Yi Luo, Tianyu Shi, Zitian Gao, Zeyu Hu, <u>Weitai Kang</u>, Qiuwu Chen</i>
              <br>
              <a href="https://arxiv.org/abs/2411.01114">PDF</a>
            </td>
          </tr>

		      <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SaCo.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              On the Faithfulness of Vision Transformer <b>Explanations</b>
              <br>
              <i>Junyi Wu, <u>Weitai Kang</u>, Hao Tang, Yuan Hong, Yan Yan</i>
              <br>
              <em style="color: red;">CVPR 2024</em>.
              <a href="https://arxiv.org/abs/2404.01415">PDF</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TokenTM.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              Token Transformation Matters: Towards Faithful Post-hoc <b>Explanation</b> for Vision Transformer
              <br>
              <i>Junyi Wu, Bin Duan, <u>Weitai Kang</u>, Hao Tang, Yan Yan</i>
              <br>
              <em style="color: red;">CVPR 2024</em>. 
              <a href="https://arxiv.org/abs/2403.14552">PDF</a>
            </td>
          </tr>
        </tbody></table></div>

	      
        
        <br>
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
<!--               <p> -->
    <ul style="width: auto; height: 350px; overflow: auto; padding-left: 20px;">
      <li style="color:black;"> <b>[02/2026]</b> My first-author paper, <a href='https://www.arxiv.org/abs/2512.11099'>VGent</a>, is accepted to <b>CVPR 2026</b> ğŸ‰.</li>

      <li style="color:black;"> <b>[01/2026]</b> My first-author paper, <a href='https://arxiv.org/abs/2508.04389'>GuirlVG</a>, is accepted to <b>ICLR 2026</b> ğŸ‰.</li>

      <li style="color:black;"> <b>[12/2025]</b> My first-author paper, <a href='https://www.arxiv.org/abs/2512.11099'>VGent</a>, received media coverage by <b>æ–°æ™ºå…ƒ</b>: <a href="https://mp.weixin.qq.com/s/R_RVJPAqzgfvn65AJYlvnw">â€œF1æš´æ¶¨20åˆ†ï¼Œæ¨ç†é€Ÿåº¦æ’å®šï¼æ–°æ¶æ„VGentï¼šå¤šç›®æ ‡å®šä½åˆå¿«åˆå‡†â€</a>. </li>

      <li style="color:black;"> <b>[12/2025]</b> My first-author paper, <a href='https://www.arxiv.org/abs/2512.11099'>VGent</a>, for <i>Multi-target & Visual Reference Visual Grounding</i> is now available on arXiv. </li>

      <li style="color:black;"> <b>[08/2025]</b> My (co-)first-author paper, <a href='https://arxiv.org/abs/2505.10887'>InfantAgent-Next</a>, is accepted to <b>NeurIPS 2025</b> ğŸ‰.</li>

      <li style="color:black;"> <b>[08/2025]</b> Our paper, <a href='https://arxiv.org/abs/2504.12599'>3DResT</a>, for <i>Semi-Supervised 3D Visual Grounding</i> is accepted to <b>TMM 2025</b> ğŸ‰.</li>
      
      <li style="color:black;"> <b>[08/2025]</b> I gave a lightning presentation of <a href='https://arxiv.org/abs/2410.00255'>Robin3D</a> at the <a href='https://www.salesforceairesearch.com/'>Salesforce AI Research Future Forum</a> at Salesforce Tower. </li>

      <li style="color:black;"> <b>[08/2025]</b> My first-author paper, <a href='https://arxiv.org/abs/2508.08066'>ExpVG</a>, for <i>Visual Grounding design in MLLM</i> is now available on arXiv. </li>

      <li style="color:black;"> <b>[08/2025]</b> My first-author paper, <a href='https://arxiv.org/abs/2508.04389'>GuirlVG</a>, for <i>GUI Visual Grounding</i> is now available on arXiv. </li>

              <li style="color:black;"> <b>[06/2025]</b> My first-author paper, <a href='https://arxiv.org/abs/2407.03243'>AttBalance</a>, is accepted to <b>ACM MM 2025</b> ğŸ‰.</li>  

              <li style="color:black;"> <b>[06/2025]</b> My first-author paper, <a href='https://arxiv.org/abs/2410.00255'>Robin3D</a>, is accepted to <b>ICCV 2025</b> ğŸ‰.</li>
              
              <li style="color:black;"> <b>[05/2025]</b> I was interviewed by <a href='https://www.mittrchina.com/news/detail/14848'>DeepTech (MIT Technology Review China)</a> to share our <a href='https://arxiv.org/abs/2505.10887'>InfantAgent-Next</a>. </li>


              <li style="color:black;"> <b>[05/2025]</b> My co-first-author paper, <a href='https://github.com/bin123apple/InfantAgent'>InfantAgent-Next</a>, for <i>AI Agent</i> is now available on arXiv and Github. </li>
              
              <li style="color:black;"> <b>[04/2025]</b> Our paper, <a href='https://arxiv.org/abs/2504.12599'>3DResT</a>, for <i>Semi-Supervised 3D RES</i> is now available on arXiv. </li>

              <li style="color:black;"> <b>[02/2025]</b> My first-author paper, <a href='https://arxiv.org/abs/2405.18295'>Intent3D</a>, received media coverage by <b>æœºå™¨ä¹‹å¿ƒ</b>: <a href="https://mp.weixin.qq.com/s/DXExxwZ7t6lzdfoKa3kJYQ">â€œICLR 2025ï½œAIä¸è¯­ï¼Œåªæ˜¯ä¸€å‘³æ ¹æ®äººç±»æ„å›¾æ¨ç†3Dç©ºé—´å®šä½â€</a>. </li>

              <li style="color:black;"> <b>[01/2025]</b> My first-author paper, <a href='https://arxiv.org/abs/2405.18295'>Intent3D</a>, is accepted to <b>ICLR 2025</b> ğŸ‰.</li>

              <li style="color:black;"> <b>[01/2025]</b> I transfer to the University of Illinois Chicago as a Ph.D. student, following my advisor, <a href='https://tomyan555.github.io/'>Prof. Yan Yan</a>.</li>

              <li style="color:black;"> <b>[2024]</b> Our paper, <a href='https://arxiv.org/abs/2411.01114'>Infant Agent</a>, for <i>AI Agent</i> is now available on arXiv. </li>

              <li style="color:black;"> <b>[2024]</b> My first-author paper, <a href='https://arxiv.org/abs/2410.00255'>Robin3D</a>, received media coverage by <b>æ–°æ™ºå…ƒ</b>: <a href="https://mp.weixin.qq.com/s/HTGo0NHy9QSPUYnTbH1ofQ">â€œç™¾ä¸‡é²æ£’æ•°æ®è®­ç»ƒï¼Œ3Dåœºæ™¯å¤§è¯­è¨€æ¨¡å‹æ–°SOTAï¼IITç­‰å‘å¸ƒRobin3Dâ€</a>. </li>

              <li style="color:black;"> <b>[2024]</b> My first-author paper, <a href='https://arxiv.org/abs/2410.00255'>Robin3D</a>, for <i>3D LLM</i> is now available on arXiv. </li>

              <li style="color:black;"> <b>[2024]</b> Our paper, <a href='https://arxiv.org/abs/2409.12963'>INTP-Video-LLM</a>, for <i>Video LLM</i> is now available on arXiv. </li>

              <li style="color:black;"> <b>[2024]</b> My first-author paper, <a href='https://arxiv.org/abs/2407.03200'>SegVG</a>, is accepted to <b>ECCV 2024</b> ğŸ‰. The <a href="https://github.com/WeitaiKang/SegVG">code</a> is now open-sourced. </li>

              <li style="color:black;"> <b>[2024]</b> Our paper, <a href="https://arxiv.org/abs/2404.01415">SaCo</a>, for <i>Transformer Explainability</i> is accepted to <b>CVPR 2024</b> ğŸ‰.</li>

              <li style="color:black;"> <b>[2024]</b> Our paper, <a href="https://arxiv.org/abs/2403.14552">TokenTM</a>, for <i>Transformer Explainability</i> is accepted to <b>CVPR 2024</b> ğŸ‰.</li>

              <li style="color:black;"> <b>[2024]</b> My first-author paper, <a href='https://arxiv.org/abs/2405.18295'>Intent3D</a>, for <i>3D Intention Grounding</i> is now available on arXiv.</li>

              <li style="color:black;"> <b>[2023]</b> My first-author paper, <a href='https://arxiv.org/abs/2407.03251'>ACTRESS</a>, for <i>Visual Grounding</i> is now available on arXiv.</li>

              <li style="color:black;"> <b>[2023]</b> I am a Teaching Assistant of CS 577: Deep Learning at Illinois Institute of Technology.</li>

              <li style="color:black;"> <b>[2023]</b> My first-author paper, <a href='https://arxiv.org/abs/2407.03200'>SegVG</a>, for <i>Visual Grounding</i> is now available on arXiv.</li>

              <li style="color:black;"> <b>[2023]</b> My first-author paper, <a href='https://arxiv.org/abs/2407.03243'>AttBalance</a>, for <i>Visual Grounding</i> constraint is now available on arXiv.</li>

              <li style="color:black;"> <b>[08/2022]</b> I join <a href='https://iitcvmlab.github.io/'>Prof. Yan Yan's group</a> as a Ph.D. student.</li>
      </ul>
          </td>
        </tr>
      </tbody></table>

      
        <table style="width:40%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=ycORK86NVoUP5s1biPMLoTUCAKwHneRSi3PHEQAggBk&cl=ffffff&w=a"></script>
      </td>
      </tr>
      </table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
                You can also reach me through WeChat: Victor_Hong_
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>


</body>

</html>
