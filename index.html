<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Weitai Kang</title>
  
  <meta name="author" content="Weitai Kang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Weitai Kang</name>
              </p>
              <p>
                I am a fourth-year Ph.D. student working with Prof. <a href='https://scholar.google.com/citations?user=zhi-j1wAAAAJ&hl=en'>Yan Yan</a> in Computer Science at the University of Illinois Chicago, expecting to graduate in 2027.
                <br><br>
                <!-- My research focuses on <b>Multimodal Learning</b>, particularly for Large Multimodal Models and Visual Grounding. -->
                I am advancing the frontier of <b>Multimodal Fine-Grained Understanding</b> across image, GUI, 3D, and video domains. To achieve this, I focus on building Multimodal Large Language Models (<a href='https://arxiv.org/abs/2410.00255'><b>Robin3D</b></a>) with optimal paradigm design (<a href='https://arxiv.org/abs/2508.08066'><b>ExpVG</b></a>) and training strategies (<a href='https://arxiv.org/abs/2508.04389'><b>GuirlVG</b></a>). I explore how to scale higher-quality data (<a href='https://arxiv.org/abs/2410.00255'><b>Robin3D</b></a>), propose stronger supervision signals (<a href='https://arxiv.org/abs/2407.03243'><b>AttBalance</b></a>, <a href='https://arxiv.org/abs/2407.03200'><b>SegVG</b></a>), and establish better benchmarks (<a href='https://arxiv.org/abs/2405.18295'><b>Intent3D</b></a>). I further work on improving overall system efficiency (<a href='https://arxiv.org/abs/2407.03251'><b>ACTRESS</b></a>, <a href='https://arxiv.org/abs/2504.12599'><b>3DResT</b></a>, <a href='https://arxiv.org/abs/2409.12963'><b>INTP-Video-LLM</b></a>), empowering AI agents (<a href='https://arxiv.org/abs/2505.10887'><b>InfantAgent-Next</b></a>), and making their decision-making mechanisms more interpretable (<a href='https://arxiv.org/abs/2404.01415'><b>SaCo</b></a>, <a href='https://arxiv.org/abs/2403.14552'><b>TokenTM</b></a>).
		<br><br>
    I have interned at Adobe, SonyAI, Tencent and SenseTime. I have been a Visiting Scholar at the University of Central Florida, working with Prof. <a href='https://scholar.google.com/citations?user=p8gsO3gAAAAJ'>Mubarak Shah</a>. Before starting my PhD, I received my bachelor's degree in Mathematics from Sun Yat-sen University in 2022, where I was awarded the Outstanding Student Scholarship each year.

		      
		<br><br>
<!-- 		<span style="color: red;">I am actively seeking 2025 Summer Internship opportunities.</span> -->
              </p>
              <p style="text-align:center">
                <a href="mailto:k13711752197@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=hDl0MkwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/weitaikang/">Linkedin</a> &nbsp/&nbsp
                <a href="https://github.com/WeitaiKang">Github</a> &nbsp/&nbsp
                <a href="https://x.com/WeitaiKang">Twitter</a> &nbsp/&nbsp
		            <a href="experience.html">Hi~</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/weitai.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/weitai.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
<!--               <p> -->
	    <ul style="width: auto; height: 250px; overflow: auto; padding-left: 20px;">
        <li style="color:black;"> <b>[08/2025]</b> My (co-)first-author paper, <a href='https://arxiv.org/abs/2505.10887'><b>InfantAgent-Next</b></a>, is accepted to <b>NeurIPS 2025</b>!!!</li>

        <li style="color:black;"> <b>[08/2025]</b> Our paper, <a href='https://arxiv.org/abs/2504.12599'><b>3DResT</b></a>, for <i>Semi-Supervised 3D Visual Grounding</i> is accepted to <b>IEEE Transactions on Multimedia</b>!!!</li>
        
        <li style="color:black;"> <b>[08/2025]</b> I presented <a href='https://arxiv.org/abs/2410.00255'><b>Robin3D</b></a> at the <a href='https://www.salesforceairesearch.com/'><b>Salesforce AI Research Future Forum</b></a> at Salesforce Tower, SF on Aug. 14th. </li>

        <li style="color:black;"> <b>[08/2025]</b> My first-author paper, <b>ExpVG</b>, for <i>Visual Grounding design in MLLM</i> is now available on <a href='https://arxiv.org/abs/2508.08066'>arXiv</a>. </li>

        <li style="color:black;"> <b>[08/2025]</b> My first-author paper, <b>GuirlVG</b>, for <i>GUI Visual Grounding</i> is now available on <a href='https://arxiv.org/abs/2508.04389'>arXiv</a>. </li>

                <li style="color:black;"> <b>[06/2025]</b> My first-author paper, <a href='https://arxiv.org/abs/2407.03243'><b>AttBalance</b></a>, is accepted to <b>ACMMM 2025</b>!!!</li>  

                <li style="color:black;"> <b>[06/2025]</b> My first-author paper, <a href='https://arxiv.org/abs/2410.00255'><b>Robin3D</b></a>, is accepted to <b>ICCV 2025</b>!!!</li>
                
                <li style="color:black;"> <b>[05/2025]</b> I was interviewed by <a href='https://www.mittrchina.com/news/detail/14848'><b>DeepTech (MIT Technology Review China)</b></a> to share our <a href='https://arxiv.org/abs/2505.10887'><b>InfantAgent-Next</b></a>. </li>


                <li style="color:black;"> <b>[05/2025]</b> My co-first-author paper, <b>InfantAgent-Next</b>, for <i>AI Agent</i> is now available on <a href='https://arxiv.org/abs/2505.10887'>arXiv</a> and <a href='https://github.com/bin123apple/InfantAgent'>Github</a>. </li>
                
                <li style="color:black;"> <b>[04/2025]</b> Our paper, <b>3DResT</b>, for <i>Semi-Supervised 3D RES</i> is now available on <a href='https://arxiv.org/abs/2504.12599'>arXiv</a>. </li>

                <li style="color:black;"> <b>[01/2025]</b> My first-author paper, <a href='https://arxiv.org/abs/2405.18295'><b>Intent3D</b></a>, is accepted to <b>ICLR 2025</b>!!!</li>

                <li style="color:black;"> <b>[01/2025]</b> I transfer to the University of Illinois Chicago as a Ph.D. student, following my advisor, <a href='https://tomyan555.github.io/'>Prof. Yan Yan</a>.</li>
        
                <li style="color:black;"> <b>[11/2024]</b> Our paper, <b>Infant Agent</b>, for <i>AI Agent</i> is now available on <a href='https://arxiv.org/abs/2411.01114'>arXiv</a>. </li>
          
                <li style="color:black;"> <b>[10/2024]</b> My first-author paper, <b>Robin3D</b>, for <i>3D LLM</i> is now available on <a href='https://arxiv.org/pdf/2410.00255'>arXiv</a>. </li>
          
                <li style="color:black;"> <b>[09/2024]</b> Our paper, <b>INTP-Video-LLM</b>, for <i>Video LLM</i> is now available on <a href='https://arxiv.org/abs/2409.12963'>arXiv</a>. </li>
		      
                <li style="color:black;"> <b>[07/2024]</b> My first-author paper, <a href='https://arxiv.org/abs/2407.03200'><b>SegVG</b></a>, is accepted to <b>ECCV 2024</b>!!! The <a href="https://github.com/WeitaiKang/SegVG">code</a> is now open-sourced. </li>
   
                <li style="color:black;"> <b>[04/2024]</b> Our paper, <b>SaCo</b>, for <i>Transformer Explainability</i> is accepted to <b>CVPR 2024</b>.</li>
                
                <li style="color:black;"> <b>[03/2024]</b> Our paper, <b>TokenTM</b>, for <i>Transformer Explainability</i> is accepted to <b>CVPR 2024</b>.</li>

                <li style="color:black;"> <b>[02/2024]</b> My first-author paper, <b>Intent3D</b>, for <i>3D Intention Grounding</i> is now available on <a href='https://arxiv.org/abs/2405.18295'>arXiv</a>.</li>
                 
                <li style="color:black;"> <b>[10/2023]</b> My first-author paper, <b>ACTRESS</b>, for <i>Visual Grounding</i> is now available on <a href='https://arxiv.org/abs/2407.03251'>arXiv</a>.</li>

                <li style="color:black;"> <b>[08/2023]</b> I am a Teaching Assistant of CS 577: Deep Learning at Illinois Institute of Technology.</li>
                
                <li style="color:black;"> <b>[04/2023]</b> My first-author paper, <b>SegVG</b>, for <i>Visual Grounding</i> is now available on <a href='https://arxiv.org/abs/2407.03200'>arXiv</a>.</li>

                <li style="color:black;"> <b>[01/2023]</b> My first-author paper, <b>AttBalance</b>, for <i>Visual Grounding</i> constraint is now available on <a href='https://arxiv.org/abs/2407.03243'>arXiv</a>.</li>

                <li style="color:black;"> <b>[08/2022]</b> I join <a href='https://iitcvmlab.github.io/'>Prof. Yan Yan's group</a> as a Ph.D. student.</li>              
<!--             </p> -->
	      </ul>
            </td>
          </tr>
        </tbody></table>


<!-- </div> -->
 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
            <!-- <p>
            </p> -->
          </td>
        </tr>
      </tbody></table>

      <div style="width: auto; height: 700px; overflow: auto; border: 1px solid #ddd;">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<!-- <tr bgcolor="#ffffd0"> -->

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/MLLMVG.png" alt="b3do" width="185" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <papertitle>ExpVG: Investigating the Design Space of Visual Grounding in Multimodal Large Language Model</papertitle>
        <br>
        <u>Weitai Kang</u>, Weiming Zhuang, Zhizhong Li, Yan Yan, Lingjuan Lyu
        <br>
        <a href="https://arxiv.org/abs/2508.08066">PDF</a>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/GuirlVG.png" alt="b3do" width="185" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <papertitle>GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning</papertitle>
        <br>
        <u>Weitai Kang</u>, Bin Lei, Gaowen Liu, Caiwen Ding, Yan Yan
        <br>
        <a href="https://arxiv.org/abs/2508.04389">PDF</a>
      </td>
    </tr>


    <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Robin3D.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning</papertitle>
              <br>
              <u>Weitai Kang</u>, Haifeng Huang, Yuzhang Shang, Mubarak Shah, Yan Yan
              <br>
              <em>ICCV</em>, 2025
	            <br>
              <a href="https://arxiv.org/pdf/2410.00255">PDF</a> /
              <a href="https://github.com/WeitaiKang/Robin3D">Code</a>
              </td>
            </tr>
		
	 <!-- <tr bgcolor="#ffffd0"> -->
    <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/intent3d.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention</papertitle>
              <br>
              <u>Weitai Kang</u>, Mengxue Qu, Jyoti Kini, Yunchao Wei, Mubarak Shah, Yan Yan
              <br>
              <em>ICLR</em>, 2025
	            <br>
              <a href="https://weitaikang.github.io/Intent3D-webpage/">Project Page</a> / 
                  <a href="https://arxiv.org/abs/2405.18295">PDF</a> / 
                  <a href="https://github.com/WeitaiKang/Intent3D">Code</a>
                      </td>
                    </tr>
          
    <tr bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/Infant-next.png" alt="b3do" width="185" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <papertitle>InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction</papertitle>
                <br>
                Bin Lei*, <u>Weitai Kang*</u>, Zijian Zhang, Winson Chen, Xi Xie, Shan Zuo, Mimi Xie, Ali Payani, Mingyi Hong, Yan Yan, Caiwen Ding
                <br>
                * Equal contribution
                <br>
                <em>NeurIPS</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2505.10887">PDF</a> /
                <a href="https://github.com/bin123apple/InfantAgent">Code</a>
              </td>
            </tr>
            
          <!-- <tr bgcolor="#ffffd0"> -->
            <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SegVG.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding</papertitle>
              <br>
              <u>Weitai Kang</u>, Gaowen Liu, Mubarak Shah, Yan Yan
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2407.03200">PDF</a> /
              <a href="https://github.com/WeitaiKang/SegVG">Code</a>
            </td>
          </tr>


          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Attbalance.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>AttBalance: Visual Grounding with Attention-Driven Constraint Balancing</papertitle>
              <br>
              <u>Weitai Kang</u>, Luowei Zhou, Junyi Wu, Changchang Sun, Yan Yan
              <br>
              <em>ACM MM</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2407.03243">PDF</a>
            </td>
          </tr>
          
	        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/INTP.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner</papertitle>
              <br>
              Yuzhang Shang, Bingxin Xu, <u>Weitai Kang</u>, Mu Cai, Yuheng Li, Zehao Wen, Zhen Dong, Kurt Keutzer, Yong Jae Lee, Yan Yan
              <br>
              <a href="https://arxiv.org/abs/2409.12963">PDF</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ACTRESS.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>ACTRESS: Active Retraining for Semi-supervised Visual Grounding</papertitle>
              <br>
              <u>Weitai Kang</u>, Mengxue Qu, Yunchao Wei, Yan Yan
              <br>
              <a href="https://arxiv.org/abs/2407.03251">PDF</a>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/3DResT.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>3DResT: A Strong Baseline for Semi-Supervised 3D Referring Expression Segmentation</papertitle>
              <br>
              Wenxin Chen, Mengxue Qu, <u>Weitai Kang</u>, Yan Yan, Yao Zhao, Yunchao Wei
              <br>
              <em>IEEE Transactions on Multimedia</em>
              <br>
              <a href="https://arxiv.org/abs/2504.12599">PDF</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Infant.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Infant Agent: A Tool-Integrated, Logic-Driven Agent with Cost-Effective API Usage</papertitle>
              <br>
              Bin Lei, Yuchen Li, Yiming Zeng, Tao Ren, Yi Luo, Tianyu Shi, Zitian Gao, Zeyu Hu, <u>Weitai Kang</u>, Qiuwu Chen
              <br>
              <a href="https://arxiv.org/abs/2411.01114">PDF</a>
            </td>
          </tr>

		      <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SaCo.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>On the Faithfulness of Vision Transformer Explanations</papertitle>
              <br>
              Junyi Wu, <u>Weitai Kang</u>, Hao Tang, Yuan Hong, Yan Yan
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2404.01415">PDF</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TokenTM.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer</papertitle>
              <br>
              Junyi Wu, Bin Duan, <u>Weitai Kang</u>, Hao Tang, Yan Yan
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.14552">PDF</a>
            </td>
          </tr>
        </tbody></table></div>

	      
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Work Experiences</heading>
          </td>
        </tr>
        </tbody></table>
        
        <!-- <div style="width: auto; height: 450px; overflow: auto; border: 1px solid #ddd;"> -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Adobe.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Adobe · Research Internship</papertitle>
              <br><br>
              Research on Large Multimodal Model.
              <br><br>
        May 2025 - Aug. 2025, San Jose, California, United States · On-site
              <br><br>
        Aug 2025 - Dec. 2025, Chicago, Illinois, United States · Remote
              <br>
              <p></p>
            </td>
          </tr>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SonyAI.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>SonyAI · Research Internship</papertitle>
              <br><br>
              Research on 2D Large Multimodal Model.
              <br><br>
        Oct. 2024 - Dec. 2024, Chicago, Illinois, United States · Remote
              <br>
              <p></p>
            </td>
          </tr>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tencent.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Tencent · Machine Learning Engineer Internship</papertitle>
              <br><br>
              Work on Human Pose Detection.
              <br><br>
        Oct. 2021 - Jul. 2022, Shenzhen, Guangdong, China · On-site
              <br>
              <p></p>
            </td>
          </tr>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sensetime.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>SenseTime · Research Internship</papertitle>
              <br><br>
              Research on Video Super-Resolution.
              <br><br>
        Jul. 2021 - Sep. 2021, Shenzhen, Guangdong, China · On-site
              <br>
              <p></p>
            </td>
          </tr>
        
        </tbody></table>

	      
        <table style="width:40%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=ycORK86NVoUP5s1biPMLoTUCAKwHneRSi3PHEQAggBk&cl=ffffff&w=a"></script>
      </td>
      </tr>
      </table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
                You can also reach me through WeChat: Victor_Hong_
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>


</body>

</html>
